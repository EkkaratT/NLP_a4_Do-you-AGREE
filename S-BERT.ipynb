{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sentence-BERT](https://arxiv.org/pdf/1908.10084.pdf)\n",
    "\n",
    "[Reference Code](https://www.pinecone.io/learn/series/nlp/train-sentence-transformers-softmax/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "from   random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set GPU device\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "# # os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test, Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'premise': Value(dtype='string', id=None),\n",
       "  'hypothesis': Value(dtype='string', id=None),\n",
       "  'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None),\n",
       "  'idx': Value(dtype='int32', id=None)},\n",
       " {'premise': Value(dtype='string', id=None),\n",
       "  'hypothesis': Value(dtype='string', id=None),\n",
       "  'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None)})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "snli = datasets.load_dataset('snli')\n",
    "mnli = datasets.load_dataset('glue', 'mnli')\n",
    "mnli['train'].features, snli['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of datasets to remove 'idx' column from\n",
    "mnli.column_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'idx' column from each dataset\n",
    "for column_names in mnli.column_names.keys():\n",
    "    mnli[column_names] = mnli[column_names].remove_columns('idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli.column_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([-1,  0,  1,  2]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(mnli['train']['label']), np.unique(snli['train']['label'])\n",
    "#snli also have -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are -1 values in the label feature, these are where no class could be decided so we remove\n",
    "snli = snli.filter(\n",
    "    lambda x: 0 if x['label'] == -1 else 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([0, 1, 2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(mnli['train']['label']), np.unique(snli['train']['label'])\n",
    "#snli also have -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you have your two DatasetDict objects named snli and mnli\n",
    "from datasets import DatasetDict\n",
    "# Merge the two DatasetDict objects\n",
    "raw_dataset = DatasetDict({\n",
    "    'train': datasets.concatenate_datasets([snli['train'], mnli['train']]).shuffle(seed=55).select(list(range(1000))),\n",
    "    'test': datasets.concatenate_datasets([snli['test'], mnli['test_mismatched']]).shuffle(seed=55).select(list(range(100))),\n",
    "    'validation': datasets.concatenate_datasets([snli['validation'], mnli['validation_mismatched']]).shuffle(seed=55).select(list(range(1000)))\n",
    "})\n",
    "#remove .select(list(range(1000))) in order to use full dataset\n",
    "# Now, merged_dataset_dict contains the combined datasets from snli and mnli\n",
    "raw_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load word2id\n",
    "with open(\"models/word2id.pkl\", \"rb\") as word2id_file:\n",
    "    word2id = pickle.load(word2id_file)\n",
    "\n",
    "# Load id2word\n",
    "with open(\"models/id2word.pkl\", \"rb\") as id2word_file:\n",
    "    id2word = pickle.load(id2word_file)\n",
    "\n",
    "# Load token_list\n",
    "with open(\"models/token_list.pkl\", \"rb\") as token_list_file:\n",
    "    token_list = pickle.load(token_list_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "max_mask = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, word2id):\n",
    "        if not isinstance(word2id, dict):\n",
    "            raise ValueError(\"word2id must be a dictionary\")\n",
    "        self.word2id = word2id\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.vocab_size = len(self.word2id)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def encode(self, sentences):\n",
    "        output = {}\n",
    "        output['input_ids'] = []\n",
    "        output['attention_mask'] = []\n",
    "        for sentence in sentences:\n",
    "            input_ids = [self.word2id.get(word, self.word2id['[UNK]']) for word in sentence.split()]\n",
    "            n_pad = self.max_len - len(input_ids)\n",
    "            input_ids.extend([0] * n_pad)\n",
    "            att_mask = [1 if idx != 0 else 0 for idx in input_ids]  # Create attention mask\n",
    "            output['input_ids'].append(torch.tensor(input_ids))  # Convert to tensor\n",
    "            output['attention_mask'].append(torch.tensor(att_mask))  # Convert to tensor\n",
    "        return output\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return ' '.join([self.id2word.get(idx.item(), '[UNK]') for idx in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    padding = max_len\n",
    "    # Tokenize the premise\n",
    "    premise_result = tokenizer.encode(\n",
    "        examples['premise'])\n",
    "    #num_rows, max_seq_length\n",
    "    # Tokenize the hypothesis\n",
    "    hypothesis_result = tokenizer.encode(\n",
    "        examples['hypothesis'])\n",
    "    #num_rows, max_seq_length\n",
    "    # Extract labels\n",
    "    labels = examples[\"label\"]\n",
    "    #num_rows\n",
    "    return {\n",
    "        \"premise_input_ids\": premise_result[\"input_ids\"],\n",
    "        \"premise_attention_mask\": premise_result[\"attention_mask\"],\n",
    "        \"hypothesis_input_ids\": hypothesis_result[\"input_ids\"],\n",
    "        \"hypothesis_attention_mask\": hypothesis_result[\"attention_mask\"],\n",
    "        \"labels\" : labels\n",
    "    }\n",
    "\n",
    "tokenized_datasets = raw_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['premise','hypothesis','label'])\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = raw_dataset['train']['premise']\n",
    "# text = [x.lower() for x in sentences] #lower case\n",
    "# text = [re.sub(\"[.,!?\\\\-]\", '', x) for x in text] #clean all symbols\n",
    "# # text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentence in text:\n",
    "#     print(sentence, \"_____\")\n",
    "#     words = sentence.split()\n",
    "#     print(words)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# # Combine everything into one to make vocab\n",
    "# word_list = list(set(\" \".join(text).split()))\n",
    "# word2id = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}  # special tokens\n",
    "\n",
    "# # Limit the word list to fit vocab_size\n",
    "# # word_list = word_list[:23068 - 4]  # Reserving space for the 4 special tokens\n",
    "\n",
    "# # Create the word2id in a single pass\n",
    "# for i, w in tqdm(enumerate(word_list), desc=\"Creating word2id\"):\n",
    "#     word2id[w] = i + 4  # because 0-3 are already occupied\n",
    "\n",
    "# # Precompute the id2word mapping (this can be done once after word2id is fully populated)\n",
    "# id2word = {v: k for k, v in word2id.items()}\n",
    "# vocab_size = len(word2id)\n",
    "# vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vocab_size = len(word2id)\n",
    "# vocab_size = 23068\n",
    "\n",
    "# # List of all tokens for the whole text\n",
    "# token_list = []\n",
    "\n",
    "# # Process sentences more efficiently\n",
    "# for sentence in tqdm(text, desc=\"Processing sentences\"):\n",
    "#     token_list.append([word2id[word] for word in sentence.split()])\n",
    "\n",
    "# # Now token_list contains the tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tokens in token_list[0]:\n",
    "#     print(id2word[tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = token_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# initialize the dataloader\n",
    "batch_size = 6\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets['validation'], \n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets['test'], \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch['premise_input_ids'].shape)\n",
    "    print(batch['premise_attention_mask'].shape)\n",
    "    print(batch['hypothesis_input_ids'].shape)\n",
    "    print(batch['hypothesis_attention_mask'].shape)\n",
    "    print(batch['labels'].shape)\n",
    "    break\n",
    "\n",
    "# the shape is [32, 128] due to batch_size is 32 and max_seq_length is 128 from setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 6\n",
    "# max_mask   = 5  # max masked tokens when 15% exceed, it will only be max_pred\n",
    "# max_len    = 1000 # maximum of length to be padded; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_batch():\n",
    "#     batch = []\n",
    "#     positive = negative = 0  #count of batch size;  we want to have half batch that are positive pairs (i.e., next sentence pairs)\n",
    "#     while positive != batch_size/2 or negative != batch_size/2:\n",
    "        \n",
    "#         #randomly choose two sentence so we can put [SEP]\n",
    "#         tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "#         #retrieve the two sentences\n",
    "#         tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "#         #1. token embedding - append CLS and SEP\n",
    "#         input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
    "\n",
    "#         #2. segment embedding - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "#         segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "#         #3. mask language modeling\n",
    "#         #masked 15%, but should be at least 1 but does not exceed max_mask\n",
    "#         n_pred =  min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
    "#         #get the pos that excludes CLS and SEP and shuffle them\n",
    "#         cand_maked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]'] and token != word2id['[SEP]']]\n",
    "#         shuffle(cand_maked_pos)\n",
    "#         masked_tokens, masked_pos = [], []\n",
    "#         #simply loop and change the input_ids to [MASK]\n",
    "#         for pos in cand_maked_pos[:n_pred]:\n",
    "#             masked_pos.append(pos)  #remember the position\n",
    "#             masked_tokens.append(input_ids[pos]) #remember the tokens\n",
    "#             #80% replace with a [MASK], but 10% will replace with a random token\n",
    "#             if random() < 0.1:  # 10%\n",
    "#                 index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "#                 input_ids[pos] = word2id[id2word[index]] # replace\n",
    "#             elif random() < 0.9:  # 80%\n",
    "#                 input_ids[pos] = word2id['[MASK]'] # make mask\n",
    "#             else:  #10% do nothing\n",
    "#                 pass\n",
    "\n",
    "#         # pad the input_ids and segment ids until the max len\n",
    "#         n_pad = max_len - len(input_ids)\n",
    "#         input_ids.extend([0] * n_pad)\n",
    "#         segment_ids.extend([0] * n_pad)\n",
    "\n",
    "#         # pad the masked_tokens and masked_pos to make sure the lenth is max_mask\n",
    "#         if max_mask > n_pred:\n",
    "#             n_pad = max_mask - n_pred\n",
    "#             masked_tokens.extend([0] * n_pad)\n",
    "#             masked_pos.extend([0] * n_pad)\n",
    "\n",
    "#         #check if first sentence is really comes before the second sentence\n",
    "#         #also make sure positive is exactly half the batch size\n",
    "#         if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
    "#             batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "#             positive += 1\n",
    "#         elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "#             batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "#             negative += 1\n",
    "            \n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchs = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #we can deconstruct using map and zip\n",
    "# input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batchs))\n",
    "# input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start from a pretrained bert-base-uncased model\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# model.load_state_dict(torch.load(\"bert_model.pth\"),strict=False)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, n_segments, d_model, device):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        #x, seg: (bs, len)\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long).to(self.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k, device):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1).to(device)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_attn_pad_mask(input_ids, input_ids, device).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, d_k, device):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(n_heads, d_model, d_k, device)\n",
    "        self.pos_ffn       = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k, device):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_k])).to(device)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / self.scale # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 6    # number of Encoder of Encoder Layer\n",
    "n_heads  = 8    # number of heads in Multi-Head Attention\n",
    "d_model  = 768  # Embedding Size\n",
    "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_k, device):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_k\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, self.d_v * n_heads)\n",
    "        self.device = device\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention(self.d_k, self.device)(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(self.n_heads * self.d_v, self.d_model, device=self.device)(context)\n",
    "        return nn.LayerNorm(self.d_model, device=self.device)(output + residual), attn # output: [batch_size x len_q x d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(F.gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, n_layers, n_heads, d_model, d_ff, d_k, n_segments, vocab_size, max_len, device):\n",
    "        super(BERT, self).__init__()\n",
    "        self.params = {'n_layers': n_layers, 'n_heads': n_heads, 'd_model': d_model,\n",
    "                       'd_ff': d_ff, 'd_k': d_k, 'n_segments': n_segments,\n",
    "                       'vocab_size': vocab_size, 'max_len': max_len}\n",
    "        self.embedding = Embedding(vocab_size, max_len, n_segments, d_model, device)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(n_heads, d_model, d_ff, d_k, device) for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        \n",
    "        # 1. predict next sentence\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled   = self.activ(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "        logits_nsp = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        # 2. predict the masked token\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked  = self.norm(F.gelu(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_nsp, output\n",
    "    \n",
    "    def get_last_hidden_state(self, input_ids, segment_ids):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids, self.device)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "n_layers = 6    # number of Encoder of Encoder Layer\n",
    "n_heads  = 12    # number of heads in Multi-Head Attention\n",
    "d_model  = 768  # Embedding Size \n",
    "d_ff = d_model * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2\n",
    "vocab_size = 6965\n",
    "max_len = 128\n",
    "\n",
    "num_epoch = 100\n",
    "model = BERT(\n",
    "    n_layers, \n",
    "    n_heads, \n",
    "    d_model, \n",
    "    d_ff, \n",
    "    d_k, \n",
    "    n_segments, \n",
    "    vocab_size, \n",
    "    max_len, \n",
    "    device\n",
    ").to(device)  # Move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(\"bert_model.pth\")\n",
    "# for name, param in checkpoint.items():\n",
    "#     print(f\"{name}: {param.shape}\")\n",
    "\n",
    "# # Print the shapes of parameters in the current model\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (embedding): Embedding(\n",
       "    (tok_embed): Embedding(6965, 768)\n",
       "    (pos_embed): Embedding(128, 768)\n",
       "    (seg_embed): Embedding(2, 768)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (enc_self_attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (pos_ffn): PoswiseFeedForwardNet(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activ): Tanh()\n",
       "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (decoder): Linear(in_features=768, out_features=6965, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model's state dict (weights)\n",
    "pretrained_weights = torch.load('models/bert_model.pth')\n",
    "\n",
    "# Load pre-trained weights into your custom model\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "# Filter out the weights that do not match (if any)\n",
    "pretrained_weights = {k: v for k, v in pretrained_weights.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
    "\n",
    "# Update the model's state_dict\n",
    "model_dict.update(pretrained_weights)\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "SBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mean pooling function\n",
    "def mean_pool(token_embeds, attention_mask):\n",
    "    # reshape attention_mask to cover 768-dimension embeddings\n",
    "    in_mask = attention_mask.unsqueeze(-1).expand(\n",
    "        token_embeds.size()\n",
    "    ).float()\n",
    "    # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
    "    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(\n",
    "        in_mask.sum(1), min=1e-9\n",
    "    )\n",
    "    return pool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Function\n",
    "\n",
    "## Classification Objective Function \n",
    "We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference  $\\lvert u - v \\rvert $ and multiply the result with the trainable weight  $ W_t ∈  \\mathbb{R}^{3n \\times k}  $:\n",
    "\n",
    "$ o = \\text{softmax}\\left(W^T \\cdot \\left(u, v, \\lvert u - v \\rvert\\right)\\right) $\n",
    "\n",
    "where $n$ is the dimension of the sentence embeddings and k the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n",
    "\n",
    "## Regression Objective Function. \n",
    "The cosine similarity between the two sentence embeddings $u$ and $v$ is computed (Figure 2). We use means quared-error loss as the objective function.\n",
    "\n",
    "(Manhatten / Euclidean distance, semantically  similar sentences can be found.)\n",
    "\n",
    "<img src=\"./figures/sbert-architecture.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configurations(u,v):\n",
    "    # build the |u-v| tensor\n",
    "    uv = torch.sub(u, v)   # batch_size,hidden_dim\n",
    "    uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
    "    \n",
    "    # concatenate u, v, |u-v|\n",
    "    x = torch.cat([u, v, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
    "    return x\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    dot_product = np.dot(u, v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    similarity = dot_product / (norm_u * norm_v)\n",
    "    return similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/sbert-ablation.png\" width=\"350\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_head = torch.nn.Linear(768*3, 3).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "optimizer_classifier = torch.optim.Adam(classifier_head.parameters(), lr=2e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st124945/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# and setup a warmup for the first ~10% steps\n",
    "total_steps = int(len(raw_dataset) / batch_size)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "\t\toptimizer, num_warmup_steps=warmup_steps,\n",
    "  \tnum_training_steps=total_steps - warmup_steps\n",
    ")\n",
    "\n",
    "# then during the training loop we update the scheduler per step\n",
    "scheduler.step()\n",
    "\n",
    "scheduler_classifier = get_linear_schedule_with_warmup(\n",
    "\t\toptimizer_classifier, num_warmup_steps=warmup_steps,\n",
    "  \tnum_training_steps=total_steps - warmup_steps\n",
    ")\n",
    "\n",
    "# then during the training loop we update the scheduler per step\n",
    "scheduler_classifier.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df49ba6d43234206a5cf890110779c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | loss = 1.457031 | Accuracy = 35.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6923a13cf04244ae4634c0797b3075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | loss = 0.915018 | Accuracy = 35.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca28b6aee4eb4da486b2f816fd74c665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | loss = 1.451059 | Accuracy = 35.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc351b26b44f4a2ab0ec33bd8683741f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | loss = 2.268982 | Accuracy = 35.3%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c173310f8f342e79556bd9324722d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | loss = 1.503545 | Accuracy = 35.3%\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "num_epoch = 5\n",
    "max_mask   = 5\n",
    "# need segment and masked for model input but not used in SBERT\n",
    "\n",
    "accuracy = 0\n",
    "count = 0\n",
    "# 1 epoch should be enough, increase if wanted\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "    classifier_head.train()\n",
    "    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, leave=True)):\n",
    "\n",
    "        # zero all gradients on each new step\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_classifier.zero_grad()\n",
    "\n",
    "        # prepare batches and more all to the active device\n",
    "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "        attention_a = batch['premise_attention_mask'].to(device)\n",
    "        attention_b = batch['hypothesis_attention_mask'].to(device)\n",
    "        label = batch['labels'].to(device)\n",
    "\n",
    "        segment_ids = torch.tensor([0] * max_len).unsqueeze(0).repeat(inputs_ids_a.shape[0], 1).to(device)\n",
    "        masked_pos  = torch.tensor([0] * max_mask).unsqueeze(0).repeat(inputs_ids_a.shape[0], 1).to(device)\n",
    "\n",
    "        # extract token embeddings from BERT at last_hidden_state\n",
    "        _,_,u = model(inputs_ids_a, segment_ids, masked_pos)\n",
    "        _,_,v = model(inputs_ids_b, segment_ids, masked_pos)\n",
    "\n",
    "\n",
    "        u_last_hidden_state = u # all token embeddings A = batch_size, seq_len, hidden_dim\n",
    "        v_last_hidden_state = v # all token embeddings B = batch_size, seq_len, hidden_dim\n",
    "\n",
    "        # print(\"u_last_hidden_state shape:\", u_last_hidden_state.shape)\n",
    "        # print(\"v_last_hidden_state shape:\", v_last_hidden_state.shape)\n",
    "\n",
    "        # print(\"attention_a shape:\", attention_a.shape)\n",
    "        # print(\"attention_b shape:\", attention_b.shape)\n",
    "\n",
    "        # u_last_hidden_state = u.get_last_hidden_state(inputs_ids_a, segment_ids) # all token embeddings A = batch_size, seq_len, hidden_dim\n",
    "        # v_last_hidden_state = v.get_last_hidden_state(inputs_ids_a, segment_ids) # all token embeddings B = batch_size, seq_len, hidden_dim\n",
    "        \n",
    "        # Extract token embeddings directly from the model output (without using get_last_hidden_state).\n",
    "        # u_last_hidden_state, u_logits_nsp = model(inputs_ids_a, segment_ids, masked_pos)\n",
    "        # v_last_hidden_state, v_logits_nsp = model(inputs_ids_b, segment_ids, masked_pos)\n",
    "        \n",
    "         # get the mean pooled vectors\n",
    "        u_mean_pool = mean_pool(u_last_hidden_state, attention_a) # batch_size, hidden_dim\n",
    "        v_mean_pool = mean_pool(v_last_hidden_state, attention_b) # batch_size, hidden_dim\n",
    "\n",
    "        # build the |u-v| tensor\n",
    "        uv = torch.sub(u_mean_pool, v_mean_pool)   # batch_size,hidden_dim\n",
    "        uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
    "\n",
    "        # concatenate u, v, |u-v|\n",
    "        x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
    "\n",
    "        # process concatenated tensor through classifier_head\n",
    "        x = classifier_head(x) #batch_size, classifer\n",
    "        for out, lab in zip(x, label):\n",
    "            count = count + 1\n",
    "            if torch.argmax(out).item() == lab.item():\n",
    "                accuracy = accuracy + 1\n",
    "        # calculate the 'softmax-loss' between predicted and true label\n",
    "        loss = criterion(x, label)\n",
    "\n",
    "        # using loss, calculate gradients and then optimizerize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_classifier.step()\n",
    "\n",
    "        scheduler.step() # update learning rate scheduler\n",
    "        scheduler_classifier.step()\n",
    "\n",
    "    print(f'Epoch: {epoch + 1} | loss = {loss.item():.6f} | Accuracy = {(accuracy / count) * 100}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to s-bert_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the model after training\n",
    "torch.save(model.state_dict(), 's-bert_model.pt')\n",
    "print(\"Model saved to s-bert_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# classifier_head.eval()\n",
    "# total_similarity = 0\n",
    "# with torch.no_grad():\n",
    "#     for step, batch in enumerate(eval_dataloader):\n",
    "#         # prepare batches and more all to the active device\n",
    "#         inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "#         inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "#         attention_a = batch['premise_attention_mask'].to(device)\n",
    "#         attention_b = batch['hypothesis_attention_mask'].to(device)\n",
    "#         label = batch['labels'].to(device)\n",
    "\n",
    "#         segment_ids_a = torch.zeros(inputs_ids_a.size(0), inputs_ids_a.size(1), dtype=torch.long).to(device)  # All 0s for sentence A\n",
    "#         segment_ids_b = torch.ones(inputs_ids_b.size(0), inputs_ids_b.size(1), dtype=torch.long).to(device)  # All 1s for sentence B\n",
    "\n",
    "#         # extract token embeddings from BERT at last_hidden_state\n",
    "#         _,_,u = model(inputs_ids_a, segment_ids_a, masked_pos)\n",
    "#         _,_,v = model(inputs_ids_b, segment_ids_b, masked_pos)\n",
    "\n",
    "#         # get the mean pooled vectors\n",
    "#         u_mean_pool = mean_pool(u, attention_a).detach().cpu().numpy().reshape(u.size(0), -1)  # Reshape to (batch_size, hidden_dim)\n",
    "#         v_mean_pool = mean_pool(v, attention_b).detach().cpu().numpy().reshape(v.size(0), -1)  # Reshape to (batch_size, hidden_dim)\n",
    "\n",
    "\n",
    "#         similarity_score = cosine_similarity(u_mean_pool, v_mean_pool)\n",
    "#         total_similarity += similarity_score\n",
    "    \n",
    "# average_similarity = total_similarity / len(eval_dataloader)\n",
    "# print(f\"Average Cosine Similarity: {average_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity: 0.9933\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "classifier_head.eval()\n",
    "total_similarity = 0\n",
    "total_count = 0  # Track the number of comparisons\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        # prepare batches and move all to the active device\n",
    "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "        attention_a = batch['premise_attention_mask'].to(device)\n",
    "        attention_b = batch['hypothesis_attention_mask'].to(device)\n",
    "        label = batch['labels'].to(device)\n",
    "\n",
    "        # Create segment ids for sentence A and sentence B\n",
    "        segment_ids_a = torch.zeros(inputs_ids_a.size(0), inputs_ids_a.size(1), dtype=torch.long).to(device)  # All 0s for sentence A\n",
    "        segment_ids_b = torch.ones(inputs_ids_b.size(0), inputs_ids_b.size(1), dtype=torch.long).to(device)  # All 1s for sentence B\n",
    "\n",
    "        # Extract token embeddings from BERT at last_hidden_state\n",
    "        _, _, u = model(inputs_ids_a, segment_ids_a, masked_pos)\n",
    "        _, _, v = model(inputs_ids_b, segment_ids_b, masked_pos)\n",
    "\n",
    "        # Get the mean pooled vectors and reshape them to 2D arrays\n",
    "        u_mean_pool = mean_pool(u, attention_a).detach().cpu().numpy().reshape(u.size(0), -1)  # (batch_size, hidden_dim)\n",
    "        v_mean_pool = mean_pool(v, attention_b).detach().cpu().numpy().reshape(v.size(0), -1)  # (batch_size, hidden_dim)\n",
    "\n",
    "        # Check if the batch sizes match\n",
    "        if u_mean_pool.shape[0] != v_mean_pool.shape[0]:\n",
    "            print(f\"Batch size mismatch: {u_mean_pool.shape[0]} vs {v_mean_pool.shape[0]}\")\n",
    "            continue  # Skip this batch if the batch sizes do not match\n",
    "\n",
    "        # Compute the cosine similarity for each pair in the batch\n",
    "        similarity_score = cosine_similarity(u_mean_pool, v_mean_pool).mean()  # Mean similarity for the batch\n",
    "        total_similarity += similarity_score\n",
    "        total_count += 1\n",
    "\n",
    "# Compute the average similarity across all batches\n",
    "average_similarity = total_similarity / total_count if total_count > 0 else 0\n",
    "print(f\"Average Cosine Similarity: {average_similarity:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9914\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(model, tokenizer, sentence_a, sentence_b, device):\n",
    "    # Tokenize and convert sentences to input IDs and attention masks\n",
    "    inputs_a = tokenizer.encode([sentence_a])\n",
    "    inputs_b = tokenizer.encode([sentence_b])\n",
    "\n",
    "    # Move input IDs and attention masks to the active device\n",
    "    inputs_ids_a = inputs_a['input_ids'][0].unsqueeze(0).to(device)\n",
    "    attention_a = inputs_a['attention_mask'][0].unsqueeze(0).to(device)\n",
    "    inputs_ids_b = inputs_b['input_ids'][0].unsqueeze(0).to(device)\n",
    "    attention_b = inputs_b['attention_mask'][0].unsqueeze(0).to(device)\n",
    "\n",
    "    # Extract token embeddings from BERT\n",
    "    u = model(inputs_ids_a, segment_ids, masked_pos)[2]\n",
    "    v = model(inputs_ids_b, segment_ids, masked_pos)[2]\n",
    "\n",
    "    # Get the mean-pooled vectors\n",
    "    u = mean_pool(u, attention_a).detach().cpu().numpy().reshape(-1)  # batch_size, hidden_dim\n",
    "    v = mean_pool(v, attention_b).detach().cpu().numpy().reshape(-1)  # batch_size, hidden_dim\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = cosine_similarity(u.reshape(1, -1), v.reshape(1, -1))[0, 0]\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "# Example usage:\n",
    "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
    "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
    "similarity = calculate_similarity(model, tokenizer, sentence_a, sentence_b, device)\n",
    "print(f\"Cosine Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
